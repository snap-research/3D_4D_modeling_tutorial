
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>CVPR'21 Tutorial on Unlocking Creativity with Computer Vision: Representations for Animation, Stylization and Manipulation</title>

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
  <link href="css/materialize.css" type="text/css" rel="stylesheet" media="screen,projection"/>
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700' rel='stylesheet' type='text/css'>
  <link href="css/style.css" rel="stylesheet" type="text/css" />
</head>

<body id="page-top">
 <div class="navbar-fixed" >
    <nav class="teal lighten-2" role="navigation">
      <div class="nav-wrapper" >
        <ul class="center hide-on-med-and-down  nav navbar-nav navbar-center">
          <li><a class="page-scroll" href="#page-top" style="color:#CD853F;font-size:20px">Home</a></li>
          <li><a class="page-scroll" href="#overview" style="color:#CD853F;font-size:20px">Overview</a></li>
          <li><a class="page-scroll" href="#organizer" style="color:#CD853F;font-size:20px">Organizer</a></li>
          <li><a class="page-scroll" href="#schedule" style="color:#CD853F;font-size:20px">Program</a></li>
          <li><a class="page-scroll" href="#speaker" style="color:#CD853F;font-size:20px">Speaker</a></li>
        </ul>
      </div>
    </nav>
  </div>
<div class="container">
  <table border="0" align="center">
    <tr>
      <td width="700" align="center" valign="middle"><h3>CVPR 2024 Tutorial on</h3>
      <span class="title">3D/4D Generation and Modeling with Generative Priors</span></td>
    </tr>
  </table>
        <h3 align="center"> <b>Date:</b>TBD</h3>
        <h3 align="center"> <b>Location::</b>TBD</h3>

        <!--h3 colspan="3" align="center"><br> Slides and recorded videos will be provided on this webpage.</h3-->
        <!-->
        <!--h3 colspan="3" align="center"><br>The tutorial can be accessed at: <a a href=https://ohyay.co/s/cvpr-tutorial-on-unlocking-creativity> this URL </a>.
        <br>
        Anyone can join! </h3-->
<!--   <p><img src="figures/teaser.jpg" width="1000" align="middle" /></p> -->
</div>

</br>

<div class="container" id="recorded-video">
  <h2>Recorded Video</h2>
  <div>
      <h3 align="center"> <b>Recored Video will be available here</b></h3>
  </div>

  <p></p>
</div>

</br>

<div class="container" id="overview">
  <h2>Overview</h2>
    <div class="overview">
    </br>
    <p>In the burgeoning metaverse landscape, where physical and digital realms converge seamlessly, the imperative to capture, represent, and analyze three-dimensional structures is paramount. The evolution of 3D and 4D generation technologies has revolutionized applications such as gaming, augmented reality (AR), and virtual reality (VR), offering unparalleled immersion and interactivity. Bridging the gap between physical and digital domains, 3D modeling enables realistic simulations, immersive gaming experiences, and augmented reality overlays. Introducing the temporal dimension further enriches these experiences, enabling lifelike animations, object tracking, and understanding of complex spatiotemporal relationships, thus reshaping our digital interactions across entertainment, education, and beyond.

<p>Traditionally, 3D generation relied on direct manipulation of 3D data, evolving alongside advancements in 2D generation techniques. Recent strides in 2D diffusion models have ushered in breakthroughs, leveraging large-scale image datasets to enhance 3D generation tasks. Methods leveraging 2D priors from diffusion models have emerged, ranging from inpainting-based approaches to knowledge distillation techniques like Score Distillation Sampling (SDS), elevating the quality and diversity of 3D asset generation. However, these approaches face limitations in scalability and realism due to biases in 2D priors and the absence of comprehensive 3D data.

<p>Challenges persist in extending 3D asset generation to scenes and mitigating biases in 2D priors for realistic synthesis in real-world settings. Addressing these issues, our tutorial delves into the burgeoning field of 3D scene generation, exploring techniques for handling diverse scene scales, compositionality, and realism. Moreover, we navigate recent advancements in 3D and 4D reconstruction from images and videos, essential for applications like augmented reality. Attendees will gain insights into the diverse paradigms of 3D/4D generation, from training on 3D data to leveraging 2D diffusion model knowledge, culminating in a comprehensive understanding of contemporary 3D modeling approaches.

<p>In conclusion, our tutorial offers a holistic exploration of 3D/4D generation and modeling, ranging from fundamental techniques to cutting-edge advancements. By navigating the intricacies of scene-level generation and leveraging 2D priors for enhanced realism, attendees will emerge equipped with a nuanced understanding of the evolving landscape of 3D modeling in the metaverse era.


    </div>
</div>

</br>

<div class="container" id="organizer">
  <h2>Organizers</h2>
  <div>

    <div class="instructor">
      <a href="https://alanspike.github.io/">
        <div class="instructorphoto"><img src="figures/hsin.png"></div>
        <div>Hsin-Ying Lee<br>Creative Vision, Snap Research</div>
      </a>
    </div>

    <div class="instructor">
      <a href="http://www.stulyakov.com/">
        <div class="instructorphoto"><img src="figures/peiye.png"></div>
        <div>Peiye Zhuang<br>Creative Vision, Snap Research</div>
      </a>
    </div>

    <div class="instructor">
      <a href="https://www.linkedin.com/in/erichuju/">
        <div class="instructorphoto"><img src="figures/chaoyang.png"></div>
        <div> Chaoyang Wang <br>Creative Vision, Snap Research</div>
      </a>
    </div>


  </div>

  <p></p>
</div>


</br>


<div class="container" id="schedule">
  <h2>Program</h2>
  <table class="program">

    <tr>
      <td width="70%">
        <p style="font-size:20px"> <b>Introduction</b> </a> </p>
      </td>
      <td width="20%"><em>Hsin-Ying Lee</em></td>
      <td width="10%"><b>08:30 - <br /> 08:40</b></td>
      <td width="10%"><a
          href="">Keynote</a> <br />
        <a href="">PDF</a>
      </td>
    </tr>


    <tr>
      <td width="70%">
        <p style="font-size:20px"> <b>3D Generation with 3D data</b> </a> </p>Introducing conventional ways of
         training 3D generation models using 3D data, including VAEs, GANs, transformers, and diffusion models.
      </td>
      <td width="20%"><em>Hsin-Ying Lee</em></td>
      <td width="10%"><b>08:40 - <br /> 09:10</b></td>
      <td width="10%"><a
          href="">Keynote</a> <br />
        <a href="">PDF</a>
      </td>
    </tr>


    <tr>
      <td>
        <p style="font-size:20px"> <b>Bridging 2D and 3D: Inpainting with Depth Estimation and Knowledge Distillation </b> </p>
        Introducing two major branches of performing 3D generation with the help of large-scale 2D diffusion models,
        including adopting 2D priors by inpainting and depth estimators, and distilling knowledge with Score Distillation Sampling (SDS)
        and its variants.

      </td>
      <td><em>Peiye Zhuang</em></td>
      <td><b>09:15 - <br /> 10:10</b></td>
      <td width="10%"><a
          href="">Keynote</a> <br />
        <a href="">PDF</a>
      </td>
    </tr>

    <tr>
      <td>
        <p style="font-size:20px"> <b>3D Scene Generation</b> </p>
        Introducing the recent advances and challenges in 3D scene generation.

      </td>
      <td><em>Hsin-Ying Lee</em></td>
      <td><b>10:15 - <br /> 11:00</b></td>
      <td width="10%"><a
          href="">Keynote</a> <br />
        <a href="">PDF</a>
      </td>
    </tr>

    <tr>
      <td>
        <p style="font-size:20px"> <b>3D and 4D Reconstruction </b>
        </p> Introducing 3D and 4D reconstruction from images and videos, and recent works leveraging generative priors including 2D diffusion models.


      </td>
      <td><em>Chaoyang Wang</em></td>
      <td><b>11:10 - <br /> 11:55</b></td>
      <td width="10%"><a
        <a href="">Keynote2</a>
        <br />
        <a href="">PDF2</a>
      </td>
    </tr>

    <td>
      <p style="font-size:20px"> <b>Closing Remarks</b></p>
    </td>
    <td><em>Hsin-Ying Lee</em></td>
    <td><b>11:55 - <br /> 12:00</b></td>


    </tr>


  </table>
</div>

</br>

<div class="container" id='speaker'>
  <h2>About the Speakers</h2>
  <div class="schedule">
    <p><b>Hsin-Ying Lee</b> is a Senior Research Scientist in the Creative Vision team at Snap Research.
      His research focuses on content generation, specifically, image/video/3D/4D generation and manipulation.
      He has published 50+ top conference papers and journals.
      Hsin-Ying got Ph.D. in the University of California, Merced.
      Before joining Snap Inc, Hsin-Ying did internships in Google and Nvidia. </p>

    <p><b>Peiye Zhuang </b> is a Research Scientist in the Creative Vision group at Snap Research.
      Her research focuses on foundation generative models and various content creation applications,
      including 2D/3D/video generation and editing. Before joining Snap, Peiye received her PhD degree in Computer Science
      at University of Illinois at Urbana-Champaign (UIUC) in 2023. She also spent time at Stanford University and interned
       with Apple, Google Brain, Facebook (now Meta), and Adobe.
      </p>

    <p><b>Chaoyang Wang </b> is a Research Scientist in the Creative Vision group at Snap Research.
      His research focuses on 3D/4D reconstruction and its application for photo-realistic novel view synthesis and
      content generation. He got his Ph.D. degree in the Robotics Institute of Carnegie Mellon University.
       Before joining Snap Inc, Chaoyang did internships in Nvidia, Adobe, Microsoft and Argo AI.
    </p>

  </div>
</div>

</br>

<div class="containersmall">
    <p>Please contact <a href="stulyakov@snap.com">Sergey Tulyakov</a> if you have question. The webpage template is by the courtesy of awesome <a href="https://gkioxari.github.io/">Georgia</a>.</p>
</div>

<!--<p align="center" class="acknowledgement">Last updated: Jan. 6, 2017</p>-->
</body>
</html>
